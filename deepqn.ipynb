{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c5e656e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from collections import deque\n",
    "import gymnasium as gym"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87386f63",
   "metadata": {},
   "outputs": [],
   "source": [
    "BUFFER_SIZE     = 10_000      # replay buffer capacity\n",
    "BATCH_SIZE      = 64          # samples per learning step\n",
    "GAMMA           = 0.99        # discount factor for future rewards\n",
    "LR              = 1e-3        # learning rate for optimizer\n",
    "TARGET_UPDATE   = 10          # how often (in episodes) to sync target network\n",
    "EPS_START       = 1.0         # initial ε for ε-greedy\n",
    "EPS_END         = 0.01        # final ε\n",
    "EPS_DECAY       = 500         # decay rate of ε (in steps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "affd136c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# replay buffer\n",
    "# breaks up correlation between sequential samples\n",
    "# smoothes out learning by sampling random batches\n",
    "class ReplayBuffer:\n",
    "    def __init__(self, capacity):\n",
    "        self.buffer = deque(maxlen=capacity)\n",
    "\n",
    "    def push(self, s, a, r, s_next, done):\n",
    "        # store transition tuple\n",
    "        self.buffer.append((s, a, r, s_next, done))\n",
    "\n",
    "    def sample(self, batch_size):\n",
    "        batch = random.sample(self.buffer, batch_size)\n",
    "        states, actions, rewards, next_states, dones = zip(*batch)\n",
    "        return (\n",
    "            torch.FloatTensor(states),\n",
    "            torch.LongTensor(actions),\n",
    "            torch.FloatTensor(rewards),\n",
    "            torch.FloatTensor(next_states),\n",
    "            torch.FloatTensor(dones),\n",
    "        )\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.buffer)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3cf1fc66",
   "metadata": {},
   "outputs": [],
   "source": [
    "class QNetwork(nn.Module):\n",
    "    def __init__(self, state_dim, action_dim):\n",
    "        super().__init__()\n",
    "        self.fc1 = nn.Linear(state_dim, 128)\n",
    "        self.fc2 = nn.Linear(128, 128)\n",
    "        self.fc3 = nn.Linear(128, action_dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = torch.relu(self.fc1(x))\n",
    "        x = torch.relu(self.fc2(x))\n",
    "        output = self.fc3(x)\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da08ee9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "env         = gym.make(\"CartPole-v1\")\n",
    "state_dim   = env.observation_space.shape[0]\n",
    "action_dim  = env.action_space.n\n",
    "\n",
    "policy_net  = QNetwork(state_dim, action_dim)\n",
    "target_net  = QNetwork(state_dim, action_dim)\n",
    "target_net.load_state_dict(policy_net.state_dict())\n",
    "target_net.eval()\n",
    "\n",
    "optimizer   = optim.Adam(policy_net.parameters(), lr=LR)\n",
    "memory      = ReplayBuffer(BUFFER_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c30c475",
   "metadata": {},
   "outputs": [],
   "source": [
    "steps_done = 0\n",
    "\n",
    "def select_action(state):\n",
    "    global steps_done\n",
    "    eps = EPS_END + (EPS_START - EPS_END) * np.exp(-1. * steps_done / EPS_DECAY)\n",
    "    steps_done += 1\n",
    "\n",
    "    if random.random() < eps:\n",
    "        return torch.tensor([[random.randrange(action_dim)]], dtype=torch.long)\n",
    "    else:\n",
    "        with torch.no_grad():\n",
    "            return policy_net(state).max(1)[1].view(1, 1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "863e6819",
   "metadata": {},
   "outputs": [],
   "source": [
    "def optimize_model():\n",
    "    if len(memory) < BATCH_SIZE:\n",
    "        return\n",
    "\n",
    "    states, actions, rewards, next_states, dones = memory.sample(BATCH_SIZE)\n",
    "\n",
    "    # 1. Compute current Q(s,a) from policy_net\n",
    "    q_values = policy_net(states).gather(1, actions.unsqueeze(1)).squeeze(1)\n",
    "\n",
    "    # 2. Double DQN:\n",
    "    #    a) action selection by policy_net\n",
    "    next_actions = policy_net(next_states).max(1)[1].unsqueeze(1)\n",
    "    #    b) value evaluation by target_net\n",
    "    next_q_values = target_net(next_states).gather(1, next_actions).squeeze(1)\n",
    "\n",
    "    # 3. Compute TD target: r + γ·Q_target(s', argmax_a Q_policy(s',a))\n",
    "    expected_q = rewards + (GAMMA * next_q_values * (1 - dones))\n",
    "\n",
    "    # 4. MSE loss & backprop\n",
    "    loss = nn.MSELoss()(q_values, expected_q.detach())\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e4734fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_episodes = 3_000\n",
    "\n",
    "for episode in range(1, num_episodes + 1):\n",
    "    # 1) Unpack reset() into obs and info\n",
    "    obs, info = env.reset()  \n",
    "\n",
    "    # 2) Turn the numpy array `obs` into a batch-shaped torch FloatTensor\n",
    "    state = torch.from_numpy(obs).float().unsqueeze(0)  # shape: [1, state_dim]\n",
    "\n",
    "    done = False\n",
    "    while not done:\n",
    "        action = select_action(state)\n",
    "        \n",
    "        # 3) Take a step: unpack the new obs plus any extra flags/info\n",
    "        next_obs, reward, terminated, truncated, info = env.step(action.item())\n",
    "        done = terminated or truncated\n",
    "\n",
    "        # 4) Convert next_obs into the same shape\n",
    "        next_state = torch.from_numpy(next_obs).float().unsqueeze(0)\n",
    "\n",
    "        \n",
    "        memory.push(\n",
    "            state.squeeze(0).numpy(),      # -> shape (state_dim,)\n",
    "            action.item(),\n",
    "            reward,\n",
    "            next_state.squeeze(0).numpy(), # -> shape (state_dim,)\n",
    "            done\n",
    "        )\n",
    "        state = next_state\n",
    "\n",
    "        optimize_model()\n",
    "\n",
    "    # 5) Periodically sync the target network\n",
    "    if episode % TARGET_UPDATE == 0:\n",
    "        target_net.load_state_dict(policy_net.state_dict())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c1bbe2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_policy(policy_net, device=\"cpu\", num_episodes=5):\n",
    "    \"\"\"\n",
    "    Run the trained policy_net for a number of episodes with human rendering enabled.\n",
    "\n",
    "    Args:\n",
    "        env          : a Gym environment created with render_mode=\"human\", e.g.\n",
    "                       env = gym.make(\"CartPole-v1\", render_mode=\"human\")\n",
    "        policy_net   : your PyTorch Q-network (policy_net)\n",
    "        device       : \"cpu\" or \"cuda\"\n",
    "        num_episodes : how many full episodes to play\n",
    "\n",
    "    Returns:\n",
    "        List of total rewards per episode.\n",
    "    \"\"\"\n",
    "    env = gym.make(\"CartPole-v1\", render_mode='human')\n",
    "    policy_net.to(device).eval()           # 1. Switch to eval mode (disable dropout/batchnorm)\n",
    "    returns = []\n",
    "\n",
    "    for ep in range(1, num_episodes + 1):\n",
    "        obs, info = env.reset()            # 2. Gymnasium-style reset returns (obs, info)\n",
    "        state = torch.from_numpy(obs)     \\\n",
    "                        .float()          \\\n",
    "                        .unsqueeze(0)     \\\n",
    "                        .to(device)      # 3. Shape [1, state_dim]\n",
    "        ep_reward = 0.0\n",
    "        done = False\n",
    "\n",
    "        while not done:\n",
    "            env.render()                   # 4. Render the current frame to the screen\n",
    "\n",
    "            with torch.no_grad():\n",
    "                # 5. Compute Q-values and pick the greedy action\n",
    "                q_values = policy_net(state)\n",
    "                action = q_values.argmax(dim=1).item()\n",
    "\n",
    "            obs, reward, terminated, truncated, info = env.step(action)\n",
    "            done = terminated or truncated\n",
    "\n",
    "            next_state = torch.from_numpy(obs) \\\n",
    "                              .float()         \\\n",
    "                              .unsqueeze(0)    \\\n",
    "                              .to(device)\n",
    "            state = next_state\n",
    "            ep_reward += reward\n",
    "\n",
    "        returns.append(ep_reward)\n",
    "        print(f\"Episode {ep:2d} ▶ Return: {ep_reward:.1f}\")\n",
    "\n",
    "    env.close()                            # 6. Clean up the render window\n",
    "    policy_net.train()                     # 7. Back to train mode if you continue training\n",
    "    avg_return = sum(returns) / len(returns)\n",
    "    print(f\"\\nAverage return over {num_episodes} episodes: {avg_return:.2f}\")\n",
    "    return returns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80c6f066",
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluate_policy(policy_net, device=\"cuda\", num_episodes=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5031c26a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f247722c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c13b7e72",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "rlagent",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
