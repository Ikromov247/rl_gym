{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d73ccb88",
   "metadata": {},
   "source": [
    "Exercise 2.5 (programming) Design and conduct an experiment to demonstrate the </br>\n",
    "difficulties that sample-average methods have for nonstationary problems. Use a modified </br>\n",
    "version of the 10-armed testbed in which all the q⇤(a) start out equal and then take</br>\n",
    "independent random walks (say by adding a normally distributed increment with mean </br>\n",
    "zero and standard deviation 0.01 to all the q⇤(a) on each step). Prepare plots like</br>\n",
    "Figure 2.2 for an action-value method using sample averages, incrementally computed,</br>\n",
    "and another action-value method using a constant step-size parameter, a = 0.1. Use </br>\n",
    "eps = 0.1 and longer runs, say of 10,000 steps."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "cada08e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8322dfaf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialize rewards\n",
    "def assign_mean_rewards(n_actions, mean, variance):\n",
    "    return np.random.normal(mean, variance, size=n_actions)\n",
    "\n",
    "def assign_equal_rewards(n_actions):\n",
    "    return np.zeros(n_actions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "32e08751",
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculate reward\n",
    "def get_reward(true_rewards, action, variance):\n",
    "    reward_mean = true_rewards[action]\n",
    "    return np.random.normal(reward_mean, variance)\n",
    "\n",
    "def get_drifting_reward(true_rewards, action):\n",
    "    \"\"\"Drift all the rewards\"\"\"\n",
    "    n = len(true_rewards)\n",
    "    drifted_rewards = true_rewards + np.random.normal(0, 0.01, size=n)\n",
    "    return drifted_rewards, true_rewards[action]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "53295a2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# policies\n",
    "def greedy(qtable):\n",
    "    return np.argmax(qtable)\n",
    "\n",
    "def epsilon_greedy(qtable, n_actions, epsilon):\n",
    "    if np.random.random() < epsilon:\n",
    "        return np.random.randint(0, n_actions)\n",
    "    else:\n",
    "        return np.argmax(qtable)\n",
    "\n",
    "def ucb(qtable:np.ndarray, counts:np.ndarray, timestamp:int, exploration_rate:float):\n",
    "    # argmax(qtable + c * ((ln(timestamp) / n)**0.5) )\n",
    "    # first try all actions once\n",
    "    zero_mask = (counts == 0)\n",
    "    if zero_mask.any():\n",
    "        # pick the first untried action\n",
    "        a = int(np.flatnonzero(zero_mask)[0])\n",
    "        counts[a] += 1\n",
    "        return a, counts\n",
    "    \n",
    "    uncertainty = (np.log(timestamp+1) / counts)**0.5\n",
    "    chosen_action  = int(\n",
    "        np.argmax(qtable + exploration_rate * uncertainty)\n",
    "    )\n",
    "    counts[chosen_action] += 1\n",
    "\n",
    "    return chosen_action, counts\n",
    "\n",
    "def gradient_bandit(preferences:np.ndarray):\n",
    "    \"\"\"Calculates probabilities of all actions based on their `preference` value\"\"\"\n",
    "    probabilities = []\n",
    "    for action in (range(len(preferences))):\n",
    "        prob = np.exp(preferences[action]) / np.sum(np.exp(preferences))\n",
    "        probabilities.append(prob)\n",
    "    return probabilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a08a0cc4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# qtable updates\n",
    "def update_estimate_average(qtable, true_reward, action, timestamp):\n",
    "    current_estimate = qtable[action]\n",
    "    # update our reward estimate by sample averaging\n",
    "    updated_estimate = current_estimate + (true_reward - current_estimate) / (timestamp + 1)\n",
    "    qtable[action] = updated_estimate\n",
    "    return None\n",
    "\n",
    "def update_estimate_alpha(qtable, true_reward, action, alpha):\n",
    "    current_estimate = qtable[action]\n",
    "    # update our reward estimate by alpha\n",
    "    updated_estimate = current_estimate + alpha * (true_reward - current_estimate)\n",
    "    qtable[action] = updated_estimate\n",
    "    return None\n",
    "\n",
    "def update_unbiased_constant(qtable, true_reward, action, timestamp, alpha):\n",
    "    # formula to calculate trace at timestamp t: `1 + (1-alpha)^t  * (trace_0 - 1)`\n",
    "    # where trace_0 is an initial constant trace value. In the textbook, it is set as 0 \n",
    "    # which leaves us with the below formula:\n",
    "    # 1 + (1-alpha) * (0-1) -> 1 + (1-alpha)^t * (-1) -> 1 - (1-alpha)^t\n",
    "    trace = 1 - (1-alpha)**timestamp\n",
    "    step_size = alpha / trace\n",
    "\n",
    "    current_estimate = qtable[action]\n",
    "    updated_estimate = current_estimate + step_size * (true_reward - current_estimate)\n",
    "    qtable[action] = updated_estimate\n",
    "    return None\n",
    "\n",
    "def update_gradient(preferences, probabilities, true_reward, action, alpha, reward_baseline):\n",
    "    # update preference for chosen action\n",
    "    for a in range(len(preferences)):\n",
    "        current_preference = preferences[a]\n",
    "        if a == action:\n",
    "            preferences[action] = current_preference + alpha * (true_reward - reward_baseline) * (1 - probabilities[a])\n",
    "        else:\n",
    "            preferences[action] = current_preference - alpha * (true_reward - reward_baseline) * probabilities[a]\n",
    "\n",
    "def update_reward_baseline(baseline, new_reward, drifting, step_size):\n",
    "    if drifting:\n",
    "        return baseline + step_size * (new_reward - baseline)\n",
    "    else:\n",
    "        return baseline + (new_reward - baseline) / (step_size + 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e38c2300",
   "metadata": {},
   "outputs": [],
   "source": [
    "# plotting\n",
    "def plot_run_results(rewards):\n",
    "    plt.figure(figsize=(12, 7))\n",
    "    plt.plot(\n",
    "        np.arange(len(rewards)), rewards\n",
    "    )\n",
    "    plt.title(\"Rewards over time\")\n",
    "    plt.show()\n",
    "\n",
    "def plot_many_run_results(title:str, **kwargs):\n",
    "    plt.figure(figsize=(12, 7))\n",
    "    \n",
    "    first_col = list(kwargs.keys())[0]\n",
    "    x_axis = np.arange(len(kwargs[first_col])) \n",
    "    \n",
    "    for name, values in kwargs.items():\n",
    "        plt.plot(\n",
    "            x_axis, values, label=name\n",
    "        )\n",
    "    \n",
    "    plt.title(title)\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "\n",
    "def plot_optimization_results(title:str, **kwargs):\n",
    "    plt.figure(figsize=(12, 7))\n",
    "    \n",
    "    policies = list(kwargs.keys())\n",
    "    \n",
    "    for policy in policies:\n",
    "        x_axis = list(kwargs[policy].keys())\n",
    "        values = list(kwargs[policy].values())\n",
    "        plt.plot(\n",
    "            x_axis, values, label=policy\n",
    "        )\n",
    "    \n",
    "    plt.title(title)\n",
    "    plt.legend()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f9b2a811",
   "metadata": {},
   "outputs": [],
   "source": [
    "def experiment(\n",
    "        narms = 10,\n",
    "        nruns = 2_000,\n",
    "        eps = 0.01,\n",
    "        alpha = 0.1,\n",
    "        timestamps = 10_000,\n",
    "        initial_reward = 0,\n",
    "        policy='epsilon', # ucb, epsilon, greedy\n",
    "        exploration_rate:int=2,\n",
    "        drifting=True,\n",
    "        variance=1,\n",
    "\n",
    "        ):\n",
    "    print(\n",
    "        f\"Running optimization for policy {policy} with parameters: {locals()}\"\n",
    "    )\n",
    "    total_rewards = []\n",
    "    total_choices = []\n",
    "\n",
    "    if policy == 'ucb':\n",
    "        counts = np.zeros(narms)\n",
    "    elif policy=='gradient':\n",
    "        reward_baseline = 0\n",
    "\n",
    "    for n in range(nruns):\n",
    "        # actual mean rewards for each action\n",
    "        rewards_table = assign_equal_rewards(narms)\n",
    "        \n",
    "        # agent's estimate\n",
    "        qtable = np.zeros(narms) + initial_reward # optimistic initialization\n",
    "        bandit_rewards = []\n",
    "        optimal_action = []\n",
    "\n",
    "        for i in range(timestamps):\n",
    "            # policies here\n",
    "            if policy == 'epsilon':\n",
    "                chosen_action = epsilon_greedy(qtable, narms, eps)\n",
    "            elif policy == 'ucb':\n",
    "                chosen_action, counts = ucb(qtable, counts, i, exploration_rate)\n",
    "            elif policy == 'gradient':\n",
    "                probabilities = gradient_bandit(qtable)\n",
    "                chosen_action = np.argmax(probabilities)\n",
    "            elif policy == 'greedy':\n",
    "                chosen_action = greedy(qtable)\n",
    "            \n",
    "            if drifting:\n",
    "                rewards_table, reward = get_drifting_reward(rewards_table, chosen_action)\n",
    "            else:\n",
    "                rewards_table, reward = get_reward(rewards_table, chosen_action, variance)\n",
    "\n",
    "            if policy == 'gradient':\n",
    "                update_gradient(qtable, probabilities, reward, chosen_action, alpha, reward_baseline)\n",
    "                # update reward baseline for non-stationary problem\n",
    "                step_size = alpha if drifting else i\n",
    "                reward_baseline = update_reward_baseline(reward_baseline, reward, drifting, step_size)\n",
    "            else:\n",
    "                update_estimate_alpha(qtable, reward, chosen_action, alpha)\n",
    "\n",
    "            optimal_action.append(\n",
    "                int(chosen_action == np.argmax(rewards_table))\n",
    "            )\n",
    "            bandit_rewards.append(reward)\n",
    "        \n",
    "        total_rewards.append(bandit_rewards)\n",
    "        total_choices.append(optimal_action)\n",
    "        if n % (nruns // 10)==0 and n > 0:\n",
    "            mean_reward = np.mean(total_rewards)\n",
    "            print(\n",
    "                f\"Mean reward: {mean_reward} \"\n",
    "            ) \n",
    "    return total_rewards, total_choices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "78009306",
   "metadata": {},
   "outputs": [],
   "source": [
    "narms = 10\n",
    "nruns = 2_000\n",
    "alpha = 0.1 # only changes for gradient bandit \n",
    "timestamps = 10_000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "03940624",
   "metadata": {},
   "outputs": [],
   "source": [
    "hyper_param_list = [2 ** x for x in range(-7, 3)]\n",
    "\n",
    "mean_rewards = {}\n",
    "mean_optimal_choices = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "be2a4a1a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running optimization for policy epsilon with parameters: {'narms': 10, 'nruns': 2000, 'eps': 0.0078125, 'alpha': 0.1, 'timestamps': 10000, 'initial_reward': 0, 'policy': 'epsilon', 'exploration_rate': 2, 'drifting': True, 'variance': 1}\n",
      "Mean reward: 0.8111447640830569 \n",
      "Mean reward: 0.8153868032260645 \n",
      "Mean reward: 0.8177093442649827 \n",
      "Mean reward: 0.8271808637877979 \n",
      "Mean reward: 0.8279553208514141 \n",
      "Mean reward: 0.8295478703577016 \n",
      "Mean reward: 0.8333767454746059 \n",
      "Mean reward: 0.8300985925715351 \n",
      "Mean reward: 0.8334377495421958 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\pc\\miniconda3\\envs\\rlagent\\Lib\\site-packages\\numpy\\_core\\fromnumeric.py:3904: RuntimeWarning: Mean of empty slice.\n",
      "  return _methods._mean(a, axis=axis, dtype=dtype,\n",
      "c:\\Users\\pc\\miniconda3\\envs\\rlagent\\Lib\\site-packages\\numpy\\_core\\_methods.py:147: RuntimeWarning: invalid value encountered in scalar divide\n",
      "  ret = ret.dtype.type(ret / rcount)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running optimization for policy epsilon with parameters: {'narms': 10, 'nruns': 2000, 'eps': 0.015625, 'alpha': 0.1, 'timestamps': 10000, 'initial_reward': 0, 'policy': 'epsilon', 'exploration_rate': 2, 'drifting': True, 'variance': 1}\n",
      "Mean reward: 0.9057598276194795 \n",
      "Mean reward: 0.8977333217102692 \n",
      "Mean reward: 0.899168649403954 \n",
      "Mean reward: 0.8944048932171763 \n",
      "Mean reward: 0.8863325178635166 \n",
      "Mean reward: 0.8830150876149863 \n",
      "Mean reward: 0.8786411823196456 \n",
      "Mean reward: 0.8767350164538087 \n",
      "Mean reward: 0.8730813763885346 \n",
      "Running optimization for policy epsilon with parameters: {'narms': 10, 'nruns': 2000, 'eps': 0.03125, 'alpha': 0.1, 'timestamps': 10000, 'initial_reward': 0, 'policy': 'epsilon', 'exploration_rate': 2, 'drifting': True, 'variance': 1}\n",
      "Mean reward: 0.9312917700120279 \n",
      "Mean reward: 0.9210762946258086 \n",
      "Mean reward: 0.915920738028263 \n",
      "Mean reward: 0.922708289973953 \n",
      "Mean reward: 0.919008394926651 \n",
      "Mean reward: 0.916245808450364 \n",
      "Mean reward: 0.9171591964699713 \n",
      "Mean reward: 0.9156614783350406 \n",
      "Mean reward: 0.9136257463075675 \n",
      "Running optimization for policy epsilon with parameters: {'narms': 10, 'nruns': 2000, 'eps': 0.0625, 'alpha': 0.1, 'timestamps': 10000, 'initial_reward': 0, 'policy': 'epsilon', 'exploration_rate': 2, 'drifting': True, 'variance': 1}\n",
      "Mean reward: 0.8892909534003767 \n",
      "Mean reward: 0.9016440923978796 \n",
      "Mean reward: 0.9097462828189181 \n",
      "Mean reward: 0.9135982541790201 \n",
      "Mean reward: 0.9080124818259513 \n",
      "Mean reward: 0.9104252575895925 \n",
      "Mean reward: 0.910224305842099 \n",
      "Mean reward: 0.9078693708286683 \n",
      "Mean reward: 0.9084738929038789 \n",
      "Running optimization for policy epsilon with parameters: {'narms': 10, 'nruns': 2000, 'eps': 0.125, 'alpha': 0.1, 'timestamps': 10000, 'initial_reward': 0, 'policy': 'epsilon', 'exploration_rate': 2, 'drifting': True, 'variance': 1}\n",
      "Mean reward: 0.8797197374404372 \n",
      "Mean reward: 0.8995511020817357 \n",
      "Mean reward: 0.8897465468098538 \n",
      "Mean reward: 0.8860612057809569 \n",
      "Mean reward: 0.8883045116667447 \n",
      "Mean reward: 0.8856673629017114 \n",
      "Mean reward: 0.8849795612425948 \n",
      "Mean reward: 0.8836822662109177 \n",
      "Mean reward: 0.8817853973040701 \n",
      "Running optimization for policy epsilon with parameters: {'narms': 10, 'nruns': 2000, 'eps': 0.25, 'alpha': 0.1, 'timestamps': 10000, 'initial_reward': 0, 'policy': 'epsilon', 'exploration_rate': 2, 'drifting': True, 'variance': 1}\n",
      "Mean reward: 0.7652070542840795 \n",
      "Mean reward: 0.7518791708016789 \n",
      "Mean reward: 0.7559908091469516 \n",
      "Mean reward: 0.755419496446636 \n",
      "Mean reward: 0.7539499071579271 \n",
      "Mean reward: 0.755202690741113 \n",
      "Mean reward: 0.7571903020088075 \n",
      "Mean reward: 0.7601135350080407 \n",
      "Mean reward: 0.7594179743145628 \n",
      "Running optimization for policy epsilon with parameters: {'narms': 10, 'nruns': 2000, 'eps': 0.5, 'alpha': 0.1, 'timestamps': 10000, 'initial_reward': 0, 'policy': 'epsilon', 'exploration_rate': 2, 'drifting': True, 'variance': 1}\n",
      "Mean reward: 0.4903744297591303 \n",
      "Mean reward: 0.49762570709150455 \n",
      "Mean reward: 0.5005408721450413 \n",
      "Mean reward: 0.5012160336620515 \n",
      "Mean reward: 0.500057297150356 \n",
      "Mean reward: 0.501737262864634 \n",
      "Mean reward: 0.5048468925116847 \n",
      "Mean reward: 0.5077694604867491 \n",
      "Mean reward: 0.5102380940881894 \n",
      "Running optimization for policy greedy with parameters: {'narms': 10, 'nruns': 2000, 'eps': None, 'alpha': 0.1, 'timestamps': 10000, 'initial_reward': 0.125, 'policy': 'greedy', 'exploration_rate': 2, 'drifting': True, 'variance': 1}\n",
      "Mean reward: 0.8022914869261605 \n",
      "Mean reward: 0.7862074398800933 \n",
      "Mean reward: 0.7801230531494427 \n",
      "Mean reward: 0.7789525520047247 \n",
      "Mean reward: 0.7750069440685987 \n",
      "Mean reward: 0.7724551965458372 \n",
      "Mean reward: 0.7722772763557663 \n",
      "Mean reward: 0.7732234278169643 \n",
      "Mean reward: 0.772729289808448 \n",
      "Running optimization for policy greedy with parameters: {'narms': 10, 'nruns': 2000, 'eps': None, 'alpha': 0.1, 'timestamps': 10000, 'initial_reward': 0.25, 'policy': 'greedy', 'exploration_rate': 2, 'drifting': True, 'variance': 1}\n",
      "Mean reward: 0.754790068342819 \n",
      "Mean reward: 0.7563562062874087 \n",
      "Mean reward: 0.7636250675101721 \n",
      "Mean reward: 0.7684945535346173 \n",
      "Mean reward: 0.7744383011471048 \n",
      "Mean reward: 0.7753447929149925 \n",
      "Mean reward: 0.7728763069825728 \n",
      "Mean reward: 0.7715877413159659 \n",
      "Mean reward: 0.7734113731098194 \n",
      "Running optimization for policy greedy with parameters: {'narms': 10, 'nruns': 2000, 'eps': None, 'alpha': 0.1, 'timestamps': 10000, 'initial_reward': 0.5, 'policy': 'greedy', 'exploration_rate': 2, 'drifting': True, 'variance': 1}\n",
      "Mean reward: 0.8194550277407657 \n",
      "Mean reward: 0.8173154528517722 \n",
      "Mean reward: 0.8041167133212465 \n",
      "Mean reward: 0.8096294465472623 \n",
      "Mean reward: 0.8088463842773101 \n",
      "Mean reward: 0.8079830269894481 \n",
      "Mean reward: 0.804403787997454 \n",
      "Mean reward: 0.8020391815020433 \n",
      "Mean reward: 0.8005827981510478 \n",
      "Running optimization for policy greedy with parameters: {'narms': 10, 'nruns': 2000, 'eps': None, 'alpha': 0.1, 'timestamps': 10000, 'initial_reward': 1, 'policy': 'greedy', 'exploration_rate': 2, 'drifting': True, 'variance': 1}\n",
      "Mean reward: 0.8116008837884927 \n",
      "Mean reward: 0.8132110785906298 \n",
      "Mean reward: 0.8155601203805926 \n",
      "Mean reward: 0.8170466232384862 \n",
      "Mean reward: 0.8153481060513884 \n",
      "Mean reward: 0.8083805604949071 \n",
      "Mean reward: 0.8108709133870691 \n",
      "Mean reward: 0.8099983273519683 \n",
      "Mean reward: 0.8122494969607505 \n",
      "Running optimization for policy greedy with parameters: {'narms': 10, 'nruns': 2000, 'eps': None, 'alpha': 0.1, 'timestamps': 10000, 'initial_reward': 2, 'policy': 'greedy', 'exploration_rate': 2, 'drifting': True, 'variance': 1}\n",
      "Mean reward: 0.8458802723376868 \n",
      "Mean reward: 0.83527565525395 \n",
      "Mean reward: 0.8397306012195572 \n",
      "Mean reward: 0.8382796232560288 \n",
      "Mean reward: 0.8447279305974033 \n",
      "Mean reward: 0.844031729760242 \n",
      "Mean reward: 0.8448926624671811 \n",
      "Mean reward: 0.845274064004926 \n",
      "Mean reward: 0.8418227942030253 \n",
      "Running optimization for policy greedy with parameters: {'narms': 10, 'nruns': 2000, 'eps': None, 'alpha': 0.1, 'timestamps': 10000, 'initial_reward': 4, 'policy': 'greedy', 'exploration_rate': 2, 'drifting': True, 'variance': 1}\n",
      "Mean reward: 0.8311719363525596 \n",
      "Mean reward: 0.8332000347725214 \n",
      "Mean reward: 0.8344798058160213 \n",
      "Mean reward: 0.8333337928727148 \n",
      "Mean reward: 0.8312242708405926 \n",
      "Mean reward: 0.8352654911047203 \n",
      "Mean reward: 0.8374726606328061 \n",
      "Mean reward: 0.8314917627121636 \n",
      "Mean reward: 0.8309468520621828 \n",
      "Running optimization for policy ucb with parameters: {'narms': 10, 'nruns': 2000, 'eps': None, 'alpha': 0.1, 'timestamps': 10000, 'initial_reward': 0, 'policy': 'ucb', 'exploration_rate': 0.25, 'drifting': True, 'variance': 1}\n",
      "Mean reward: 0.7243905518845992 \n",
      "Mean reward: 0.7221961959278396 \n",
      "Mean reward: 0.7272325909205474 \n",
      "Mean reward: 0.7226471787904777 \n",
      "Mean reward: 0.7236026922698947 \n",
      "Mean reward: 0.731512839237218 \n",
      "Mean reward: 0.7265769797646605 \n",
      "Mean reward: 0.7267023682637809 \n",
      "Mean reward: 0.7258445383935744 \n",
      "Running optimization for policy ucb with parameters: {'narms': 10, 'nruns': 2000, 'eps': None, 'alpha': 0.1, 'timestamps': 10000, 'initial_reward': 0, 'policy': 'ucb', 'exploration_rate': 0.5, 'drifting': True, 'variance': 1}\n",
      "Mean reward: 0.7308340250460467 \n",
      "Mean reward: 0.7262555004196869 \n",
      "Mean reward: 0.707828576467261 \n",
      "Mean reward: 0.7229904895832079 \n",
      "Mean reward: 0.7183783897560221 \n",
      "Mean reward: 0.7192566403498952 \n",
      "Mean reward: 0.7164011929568329 \n",
      "Mean reward: 0.7148382431092416 \n",
      "Mean reward: 0.7186234801099112 \n",
      "Running optimization for policy ucb with parameters: {'narms': 10, 'nruns': 2000, 'eps': None, 'alpha': 0.1, 'timestamps': 10000, 'initial_reward': 0, 'policy': 'ucb', 'exploration_rate': 1, 'drifting': True, 'variance': 1}\n",
      "Mean reward: 0.7153100428898104 \n",
      "Mean reward: 0.7204591302007975 \n",
      "Mean reward: 0.7145499089080863 \n",
      "Mean reward: 0.7203146410362222 \n",
      "Mean reward: 0.7159306270360122 \n",
      "Mean reward: 0.7151037940625036 \n",
      "Mean reward: 0.7136379228158132 \n",
      "Mean reward: 0.7170644030810923 \n",
      "Mean reward: 0.7167800003849495 \n",
      "Running optimization for policy ucb with parameters: {'narms': 10, 'nruns': 2000, 'eps': None, 'alpha': 0.1, 'timestamps': 10000, 'initial_reward': 0, 'policy': 'ucb', 'exploration_rate': 2, 'drifting': True, 'variance': 1}\n",
      "Mean reward: 0.711907135980177 \n",
      "Mean reward: 0.700984452700216 \n",
      "Mean reward: 0.7143362125712636 \n",
      "Mean reward: 0.7132365796352078 \n",
      "Mean reward: 0.7173346839888387 \n",
      "Mean reward: 0.7228858700089358 \n",
      "Mean reward: 0.7201165126606265 \n",
      "Mean reward: 0.7238140917093571 \n",
      "Mean reward: 0.7251716118659026 \n",
      "Running optimization for policy ucb with parameters: {'narms': 10, 'nruns': 2000, 'eps': None, 'alpha': 0.1, 'timestamps': 10000, 'initial_reward': 0, 'policy': 'ucb', 'exploration_rate': 4, 'drifting': True, 'variance': 1}\n",
      "Mean reward: 0.7476436421272418 \n",
      "Mean reward: 0.7267280355366194 \n",
      "Mean reward: 0.7215210748703561 \n",
      "Mean reward: 0.7211694145746301 \n",
      "Mean reward: 0.721731833644058 \n",
      "Mean reward: 0.7274046881871585 \n",
      "Mean reward: 0.7257791826773096 \n",
      "Mean reward: 0.7228467124483949 \n",
      "Mean reward: 0.7223389770466165 \n",
      "Running optimization for policy gradient with parameters: {'narms': 10, 'nruns': 2000, 'eps': None, 'alpha': 0.0078125, 'timestamps': 10000, 'initial_reward': 1, 'policy': 'gradient', 'exploration_rate': 2, 'drifting': True, 'variance': 1}\n",
      "Mean reward: -0.5205063464813313 \n",
      "Mean reward: -0.5073148883136521 \n",
      "Mean reward: -0.4870267287941911 \n",
      "Mean reward: -0.478552211018491 \n",
      "Mean reward: -0.4761661641481928 \n",
      "Mean reward: -0.4757440985705417 \n",
      "Mean reward: -0.48366948225340123 \n",
      "Mean reward: -0.4838303663561949 \n",
      "Mean reward: -0.4836777357499363 \n",
      "Running optimization for policy gradient with parameters: {'narms': 10, 'nruns': 2000, 'eps': None, 'alpha': 0.015625, 'timestamps': 10000, 'initial_reward': 1, 'policy': 'gradient', 'exploration_rate': 2, 'drifting': True, 'variance': 1}\n",
      "Mean reward: -0.5295089638181277 \n",
      "Mean reward: -0.5106546142128887 \n",
      "Mean reward: -0.49763644464671947 \n",
      "Mean reward: -0.4947096938735292 \n",
      "Mean reward: -0.49392452984081225 \n",
      "Mean reward: -0.4931161376932547 \n",
      "Mean reward: -0.49230275539082796 \n",
      "Mean reward: -0.49046548980152377 \n",
      "Mean reward: -0.49005999527773253 \n",
      "Running optimization for policy gradient with parameters: {'narms': 10, 'nruns': 2000, 'eps': None, 'alpha': 0.03125, 'timestamps': 10000, 'initial_reward': 1, 'policy': 'gradient', 'exploration_rate': 2, 'drifting': True, 'variance': 1}\n",
      "Mean reward: -0.5691167053005879 \n",
      "Mean reward: -0.5595942750818342 \n",
      "Mean reward: -0.5506964138459209 \n",
      "Mean reward: -0.5528952081931214 \n",
      "Mean reward: -0.5441728527281421 \n",
      "Mean reward: -0.5349213364828181 \n",
      "Mean reward: -0.5335260360687715 \n",
      "Mean reward: -0.5356390371752655 \n",
      "Mean reward: -0.537772188272724 \n",
      "Running optimization for policy gradient with parameters: {'narms': 10, 'nruns': 2000, 'eps': None, 'alpha': 0.0625, 'timestamps': 10000, 'initial_reward': 1, 'policy': 'gradient', 'exploration_rate': 2, 'drifting': True, 'variance': 1}\n",
      "Mean reward: -0.5919684486780138 \n",
      "Mean reward: -0.5477120877023842 \n",
      "Mean reward: -0.5471194359720062 \n",
      "Mean reward: -0.5421316170825485 \n",
      "Mean reward: -0.5440223536604847 \n",
      "Mean reward: -0.5491694834746651 \n",
      "Mean reward: -0.5471764419965902 \n",
      "Mean reward: -0.547527888025061 \n",
      "Mean reward: -0.5511054217971156 \n",
      "Running optimization for policy gradient with parameters: {'narms': 10, 'nruns': 2000, 'eps': None, 'alpha': 0.125, 'timestamps': 10000, 'initial_reward': 1, 'policy': 'gradient', 'exploration_rate': 2, 'drifting': True, 'variance': 1}\n",
      "Mean reward: -0.5489672131938099 \n",
      "Mean reward: -0.5677016784087547 \n",
      "Mean reward: -0.5565407148308126 \n",
      "Mean reward: -0.5645433319977178 \n",
      "Mean reward: -0.5663614667792329 \n",
      "Mean reward: -0.5606359416935329 \n",
      "Mean reward: -0.5651209540940374 \n",
      "Mean reward: -0.5703342064352256 \n",
      "Mean reward: -0.5712135843546697 \n",
      "Running optimization for policy gradient with parameters: {'narms': 10, 'nruns': 2000, 'eps': None, 'alpha': 0.25, 'timestamps': 10000, 'initial_reward': 1, 'policy': 'gradient', 'exploration_rate': 2, 'drifting': True, 'variance': 1}\n",
      "Mean reward: -0.5804008811839523 \n",
      "Mean reward: -0.5686658049172703 \n",
      "Mean reward: -0.5732016551150698 \n",
      "Mean reward: -0.5856695228522197 \n",
      "Mean reward: -0.5855094733124447 \n",
      "Mean reward: -0.5839627527148172 \n",
      "Mean reward: -0.586787544473721 \n",
      "Mean reward: -0.5878286952145056 \n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[10]\u001b[39m\u001b[32m, line 83\u001b[39m\n\u001b[32m     80\u001b[39m mean_optimal_choices[policy] = {}\n\u001b[32m     81\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m alpha \u001b[38;5;129;01min\u001b[39;00m hyper_param_list:\n\u001b[32m---> \u001b[39m\u001b[32m83\u001b[39m     total_rewards, total_choices = \u001b[43mexperiment\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m     84\u001b[39m \u001b[43m        \u001b[49m\u001b[43mnarms\u001b[49m\u001b[43m=\u001b[49m\u001b[43mnarms\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     85\u001b[39m \u001b[43m        \u001b[49m\u001b[43mnruns\u001b[49m\u001b[43m=\u001b[49m\u001b[43mnruns\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     86\u001b[39m \u001b[43m        \u001b[49m\u001b[43meps\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m     87\u001b[39m \u001b[43m        \u001b[49m\u001b[43malpha\u001b[49m\u001b[43m=\u001b[49m\u001b[43malpha\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     88\u001b[39m \u001b[43m        \u001b[49m\u001b[43mtimestamps\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtimestamps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     89\u001b[39m \u001b[43m        \u001b[49m\u001b[43mpolicy\u001b[49m\u001b[43m=\u001b[49m\u001b[43mpolicy\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     90\u001b[39m \u001b[43m        \u001b[49m\u001b[43minitial_reward\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;66;43;03m# just to avoid a 0 \u001b[39;49;00m\n\u001b[32m     91\u001b[39m \n\u001b[32m     92\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     94\u001b[39m     \u001b[38;5;66;03m# only consider the second half of timestamps for reward calculation\u001b[39;00m\n\u001b[32m     95\u001b[39m     mean_reward = np.mean(total_rewards[timestamps//\u001b[32m2\u001b[39m:]) \n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[7]\u001b[39m\u001b[32m, line 47\u001b[39m, in \u001b[36mexperiment\u001b[39m\u001b[34m(narms, nruns, eps, alpha, timestamps, initial_reward, policy, exploration_rate, drifting, variance)\u001b[39m\n\u001b[32m     44\u001b[39m     chosen_action = greedy(qtable)\n\u001b[32m     46\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m drifting:\n\u001b[32m---> \u001b[39m\u001b[32m47\u001b[39m     rewards_table, reward = \u001b[43mget_drifting_reward\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrewards_table\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mchosen_action\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     48\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m     49\u001b[39m     rewards_table, reward = get_reward(rewards_table, chosen_action, variance)\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[3]\u001b[39m\u001b[32m, line 9\u001b[39m, in \u001b[36mget_drifting_reward\u001b[39m\u001b[34m(true_rewards, action)\u001b[39m\n\u001b[32m      7\u001b[39m \u001b[38;5;250m\u001b[39m\u001b[33;03m\"\"\"Drift all the rewards\"\"\"\u001b[39;00m\n\u001b[32m      8\u001b[39m n = \u001b[38;5;28mlen\u001b[39m(true_rewards)\n\u001b[32m----> \u001b[39m\u001b[32m9\u001b[39m drifted_rewards = true_rewards + \u001b[43mnp\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrandom\u001b[49m\u001b[43m.\u001b[49m\u001b[43mnormal\u001b[49m\u001b[43m(\u001b[49m\u001b[32;43m0\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[32;43m0.01\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msize\u001b[49m\u001b[43m=\u001b[49m\u001b[43mn\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     10\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m drifted_rewards, true_rewards[action]\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "policy = 'epsilon'\n",
    "mean_rewards[policy] = {}\n",
    "mean_optimal_choices[policy] = {}\n",
    "\n",
    "for eps in hyper_param_list:\n",
    "    if eps >= 1:\n",
    "        break\n",
    "    total_rewards, total_choices = experiment(\n",
    "        narms=narms,\n",
    "        nruns=nruns,\n",
    "        eps=eps, # <-- hyper-parameter\n",
    "        alpha=alpha,\n",
    "        timestamps=timestamps,\n",
    "        policy=policy,\n",
    "\n",
    "    )\n",
    "    # only consider the second half of timestamps for reward calculation\n",
    "    mean_reward = np.mean(total_rewards[timestamps//2:]) \n",
    "    optimal_choice = np.mean(total_choices[timestamps//2:], axis=0) * 100.0\n",
    "\n",
    "    mean_rewards[policy][eps] = mean_reward\n",
    "    mean_optimal_choices[policy][eps] = mean_reward\n",
    "\n",
    "# greedy\n",
    "policy = 'greedy'\n",
    "mean_rewards[policy] = {}\n",
    "mean_optimal_choices[policy] = {}\n",
    "\n",
    "for initial_reward in hyper_param_list:\n",
    "    if initial_reward < 0.125:\n",
    "        continue\n",
    "    total_rewards, total_choices = experiment(\n",
    "        narms=narms,\n",
    "        nruns=nruns,\n",
    "        eps=None,\n",
    "        alpha=alpha,\n",
    "        timestamps=timestamps,\n",
    "        policy=policy,\n",
    "        initial_reward=initial_reward,\n",
    "        \n",
    "    )\n",
    "\n",
    "    # only consider the second half of timestamps for reward calculation\n",
    "    mean_reward = np.mean(total_rewards[timestamps//2:]) \n",
    "    optimal_choice = np.mean(total_choices[timestamps//2:], axis=0) * 100.0\n",
    "\n",
    "    mean_rewards[policy][initial_reward] = mean_reward\n",
    "    mean_optimal_choices[policy][initial_reward] = mean_reward\n",
    "\n",
    "\n",
    "# ucb\n",
    "policy = 'ucb'\n",
    "mean_rewards[policy] = {}\n",
    "mean_optimal_choices[policy] = {}\n",
    "for exploration_rate in hyper_param_list:\n",
    "    if exploration_rate < 0.25:\n",
    "        continue\n",
    "    total_rewards, total_choices = experiment(\n",
    "        narms=narms,\n",
    "        nruns=nruns,\n",
    "        eps=None,\n",
    "        alpha=alpha,\n",
    "        timestamps=timestamps,\n",
    "        policy=policy,\n",
    "        exploration_rate=exploration_rate,\n",
    "        \n",
    "    )\n",
    "\n",
    "    # only consider the second half of timestamps for reward calculation\n",
    "    mean_reward = np.mean(total_rewards[timestamps//2:]) \n",
    "    optimal_choice = np.mean(total_choices[timestamps//2:], axis=0) * 100.0\n",
    "\n",
    "    mean_rewards[policy][exploration_rate] = mean_reward\n",
    "    mean_optimal_choices[policy][exploration_rate] = mean_reward\n",
    "\n",
    "\n",
    "# gradient\n",
    "policy = 'gradient'\n",
    "mean_rewards[policy] = {}\n",
    "mean_optimal_choices[policy] = {}\n",
    "for alpha in hyper_param_list:\n",
    "\n",
    "    total_rewards, total_choices = experiment(\n",
    "        narms=narms,\n",
    "        nruns=nruns,\n",
    "        eps=None,\n",
    "        alpha=alpha,\n",
    "        timestamps=timestamps,\n",
    "        policy=policy,\n",
    "        initial_reward=1, # just to avoid a 0 \n",
    "\n",
    "    )\n",
    "\n",
    "    # only consider the second half of timestamps for reward calculation\n",
    "    mean_reward = np.mean(total_rewards[timestamps//2:]) \n",
    "    optimal_choice = np.mean(total_choices[timestamps//2:], axis=0) * 100.0\n",
    "\n",
    "    mean_rewards[policy][alpha] = mean_reward\n",
    "    mean_optimal_choices[policy][alpha] = mean_reward\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6fdfcbd2",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_optimization_results(\n",
    "    title='optimization comparison',\n",
    "    **mean_rewards\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d05b701b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0d3d952",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "rlagent",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
