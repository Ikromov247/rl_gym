{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "fe913e31",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c2e93fcd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gymnasium as gym"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1965c93",
   "metadata": {},
   "source": [
    "The classic cart pole problem is an introduction to Reinforcement Learning. \\\n",
    "An agent learns to balance a pole on a cart by moving it left or right "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9b42b2ce",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([ 0.0273956 , -0.00611216,  0.03585979,  0.0197368 ], dtype=float32),\n",
       " {})"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env = gym.make('CartPole-v1', render_mode=None)\n",
    "env.reset(seed=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a1e38df",
   "metadata": {},
   "source": [
    "The observation space includes the cart position and velocity. \\\n",
    "It also includes the pole's angle (in radians) and velocity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "04a97096",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Observation space: Box([-4.8               -inf -0.41887903        -inf], [4.8               inf 0.41887903        inf], (4,), float32)\n"
     ]
    }
   ],
   "source": [
    "print(f\"Observation space: {env.observation_space}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8a9e89d",
   "metadata": {},
   "source": [
    "The agent has an action space which specifies what it can do \\\n",
    "It can move left or right \\\n",
    "to balance the angle and velocity of the pole."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d6d5012a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Action space: Discrete(2)\n"
     ]
    }
   ],
   "source": [
    "print(f\"Action space: {env.action_space}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9554465",
   "metadata": {},
   "source": [
    "## The Environment"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10c929b1",
   "metadata": {},
   "source": [
    "Here, we let the agent move 100 times \\\n",
    "At the end, we print its final position, velocity and the pole's angle and velocity "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e10b56c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# min_cart_vel, max_cart_vel = float('inf'), float('-inf')\n",
    "# min_pole_vel, max_pole_vel = float('inf'), float('-inf')\n",
    "\n",
    "# for _ in range(1000):  # Run 1000 random steps\n",
    "#     action = env.action_space.sample()\n",
    "#     observation, _, done, _, _ = env.step(action)\n",
    "    \n",
    "#     cart_vel, pole_vel = observation[1], observation[3]\n",
    "#     min_cart_vel = min(min_cart_vel, cart_vel)\n",
    "#     max_cart_vel = max(max_cart_vel, cart_vel)\n",
    "#     min_pole_vel = min(min_pole_vel, pole_vel)\n",
    "#     max_pole_vel = max(max_pole_vel, pole_vel)\n",
    "    \n",
    "#     if done:\n",
    "#         observation, _ = env.reset()\n",
    "\n",
    "# print(f\"Cart velocity range: ({min_cart_vel:.2f}, {max_cart_vel:.2f})\")\n",
    "# print(f\"Pole velocity range: ({min_pole_vel:.2f}, {max_pole_vel:.2f})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc90237d",
   "metadata": {},
   "source": [
    "## The State Discretization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19fc7708",
   "metadata": {},
   "source": [
    "The action state originally involves continuous values. \\\n",
    "We will turn those into discrete values to limit the number of actions an agent can take \\\n",
    "It will make learning easier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "bbd5dca4",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_buckets = (5, 8, 10, 5) # cart position, cart velocity, pole angle, pole velocity\n",
    "\n",
    "state_bounds = list(zip(env.observation_space.low, env.observation_space.high))\n",
    "state_bounds[1] = (-1.5, 1.5) # cart velocity bounds\n",
    "state_bounds[3] = (-2.0, 2.0) # pole velocity bounds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e5c51658",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(np.float32(-4.8), np.float32(4.8)),\n",
       " (-1.5, 1.5),\n",
       " (np.float32(-0.41887903), np.float32(0.41887903)),\n",
       " (-2.0, 2.0)]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "state_bounds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "a1fae2e6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "EnvSpec(id='CartPole-v1', entry_point='gymnasium.envs.classic_control.cartpole:CartPoleEnv', reward_threshold=475.0, nondeterministic=False, max_episode_steps=None, order_enforce=False, disable_env_checker=True, kwargs={'render_mode': None}, namespace=None, name='CartPole', version=1, additional_wrappers=(), vector_entry_point='gymnasium.envs.classic_control.cartpole:CartPoleVectorEnv')"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env.unwrapped.spec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6c789a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def discretize_state(observation):\n",
    "    \"\"\"\n",
    "    Convert a continuous state to its discrete counterpart\n",
    "    \"\"\"\n",
    "    discretized = []\n",
    "    for i, (lower, upper) in enumerate(state_bounds):\n",
    "        if upper == float('inf'):\n",
    "            upper = env.unwrapped.spec.kwargs.get('thresholds', [2.4, 2.4, 0.418, 0.418])[i]\n",
    "        if lower == float('-inf'):\n",
    "            lower = -upper\n",
    "        \n",
    "        scaling = (n_buckets[i] - 1) / (upper - lower)\n",
    "        new_obs = int(\n",
    "            np.floor(scaling * (observation[i] - lower))\n",
    "            )\n",
    "        new_obs = min(n_buckets[i] - 1, max(0, new_obs))\n",
    "        discretized.append(new_obs)\n",
    "    return tuple(discretized)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c848969",
   "metadata": {},
   "source": [
    "## Q-Table initialization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d741b1c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_q_table():\n",
    "    \"\"\"\n",
    "    Create and initialize Q-table with zeros\n",
    "    \"\"\"\n",
    "    q_table_shape = n_buckets + (env.action_space.n,)\n",
    "    q_table = np.zeros(q_table_shape)\n",
    "    return q_table"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e520115b",
   "metadata": {},
   "source": [
    "## Epsilon-greedy action selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "525e6ac7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def select_action(state, q_table, epsilon):\n",
    "    \"\"\"\n",
    "    Select an action using epsilon-greedy policy\n",
    "    \"\"\"\n",
    "    if np.random.random() < epsilon:\n",
    "        # explore: select a random action\n",
    "        return env.action_space.sample()\n",
    "    else:\n",
    "        # exploit: select action with the highest q-value\n",
    "        return np.argmax(q_table[state])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4cc9937",
   "metadata": {},
   "source": [
    "## Q-learning update rule"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "4ffd0f89",
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_q_value(state, action, reward, next_state, q_table, alpha, gamma):\n",
    "    \"\"\"\n",
    "    Update Q-value for a state-action pair\n",
    "    \"\"\"\n",
    "    next_max_q = np.max(q_table[next_state])\n",
    "    # current q = 100\n",
    "    # next max q = 70\n",
    "    # reward 20\n",
    "    current_q = q_table[state][action]\n",
    "    new_q = current_q + alpha * (reward + gamma * next_max_q - current_q)\n",
    "    q_table[state][action] = new_q"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24c56592",
   "metadata": {},
   "source": [
    "## Training Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "a90535cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_agent(n_episodes, alpha, gamma, epsilon_start, epsilon_end, epsilon_decay):\n",
    "    \"\"\"\n",
    "    Train the Q-learning agent\n",
    "    \"\"\"\n",
    "    q_table = create_q_table()\n",
    "\n",
    "    total_episode_states = []\n",
    "\n",
    "    for episode in range(n_episodes):\n",
    "        episode_state = {}\n",
    "\n",
    "        # Reduce epsilon (exploration rate) over time\n",
    "        epsilon = epsilon_end + (epsilon_start - epsilon_end) * np.exp(-episode / epsilon_decay)\n",
    "        episode_state['epsilon'] = epsilon\n",
    "\n",
    "        # reset environment\n",
    "        observation, info = env.reset()\n",
    "        state = discretize_state(observation)\n",
    "        \n",
    "        done = False\n",
    "        total_reward = 0\n",
    "        episode_length = 0\n",
    "        # One episode of training\n",
    "        while not done:\n",
    "            action = select_action(state, q_table, epsilon)\n",
    "\n",
    "            # Take action and observe the result\n",
    "            next_observation, reward, terminated, truncated, info = env.step(action)\n",
    "            done = terminated or truncated\n",
    "            next_state = discretize_state(next_observation)\n",
    "\n",
    "            update_q_value(state, action, reward, next_state, q_table, alpha, gamma)\n",
    "\n",
    "            # move to the next state\n",
    "            state = next_state\n",
    "            total_reward += reward\n",
    "            episode_length += 1\n",
    "        \n",
    "        episode_state['total_reward'] = total_reward\n",
    "        episode_state['episode_length'] = episode_length\n",
    "        total_episode_states.append(episode_state)\n",
    "\n",
    "        if episode % 50 == 0:\n",
    "            last_50_episode_results = total_episode_states[episode-50:]\n",
    "            mean_reward = sum([past_episode['total_reward'] for past_episode in last_50_episode_results]) / 50\n",
    "            print(f\"Episode {episode}, Average reward: {mean_reward}, Epsilon: {epsilon:.4f}\")\n",
    "\n",
    "        \n",
    "\n",
    "    return q_table, total_episode_states"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6dd93b9c",
   "metadata": {},
   "source": [
    "## Testing the trained agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "58717412",
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_agent(q_table, n_episodes=10, render=True):\n",
    "    \"\"\"\n",
    "    Test the trained Q-learning agent over several episodes\n",
    "    \n",
    "    Args:\n",
    "        q_table: The learned Q-table\n",
    "        n_episodes: Number of test episodes to run\n",
    "        render: Whether to render the environment (set to True to visualize)\n",
    "    \n",
    "    Returns:\n",
    "        Average episode length across all test episodes\n",
    "    \"\"\"\n",
    "    env_test = gym.make('CartPole-v1', render_mode='human' if render else None)\n",
    "    episode_lengths = []\n",
    "    \n",
    "    for episode in range(n_episodes):\n",
    "        # Reset the environment\n",
    "        observation, info = env_test.reset()\n",
    "        state = discretize_state(observation)\n",
    "        \n",
    "        done = False\n",
    "        episode_length = 0\n",
    "        \n",
    "        # Run one episode\n",
    "        while not done:\n",
    "            # Always select the best action (no exploration)\n",
    "            action = np.argmax(q_table[state])\n",
    "            \n",
    "            # Take action\n",
    "            next_observation, reward, terminated, truncated, info = env_test.step(action)\n",
    "            done = terminated or truncated\n",
    "            \n",
    "            # Update state and counter\n",
    "            state = discretize_state(next_observation)\n",
    "            episode_length += 1\n",
    "            \n",
    "        episode_lengths.append(episode_length)\n",
    "        print(f\"Test Episode {episode+1}/{n_episodes}, Length: {episode_length}\")\n",
    "    \n",
    "    avg_length = sum(episode_lengths) / len(episode_lengths)\n",
    "    print(f\"Average episode length: {avg_length:.2f}\")\n",
    "    \n",
    "    env_test.close()\n",
    "    return avg_length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "a41fcbfd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 0, Average reward: 0.34, Epsilon: 1.0000\n",
      "Episode 50, Average reward: 23.48, Epsilon: 0.9836\n",
      "Episode 100, Average reward: 24.34, Epsilon: 0.9675\n",
      "Episode 150, Average reward: 20.62, Epsilon: 0.9517\n",
      "Episode 200, Average reward: 22.16, Epsilon: 0.9362\n",
      "Episode 250, Average reward: 21.28, Epsilon: 0.9208\n",
      "Episode 300, Average reward: 24.1, Epsilon: 0.9058\n",
      "Episode 350, Average reward: 26.38, Epsilon: 0.8910\n",
      "Episode 400, Average reward: 21.94, Epsilon: 0.8764\n",
      "Episode 450, Average reward: 23.24, Epsilon: 0.8621\n",
      "Episode 500, Average reward: 22.2, Epsilon: 0.8480\n",
      "Episode 550, Average reward: 21.42, Epsilon: 0.8342\n",
      "Episode 600, Average reward: 25.04, Epsilon: 0.8205\n",
      "Episode 650, Average reward: 24.74, Epsilon: 0.8071\n",
      "Episode 700, Average reward: 26.52, Epsilon: 0.7940\n",
      "Episode 750, Average reward: 24.16, Epsilon: 0.7810\n",
      "Episode 800, Average reward: 24.46, Epsilon: 0.7683\n",
      "Episode 850, Average reward: 30.36, Epsilon: 0.7557\n",
      "Episode 900, Average reward: 25.2, Epsilon: 0.7434\n",
      "Episode 950, Average reward: 31.82, Epsilon: 0.7313\n",
      "Episode 1000, Average reward: 29.16, Epsilon: 0.7194\n",
      "Episode 1050, Average reward: 28.0, Epsilon: 0.7076\n",
      "Episode 1100, Average reward: 29.66, Epsilon: 0.6961\n",
      "Episode 1150, Average reward: 25.52, Epsilon: 0.6848\n",
      "Episode 1200, Average reward: 31.64, Epsilon: 0.6736\n",
      "Episode 1250, Average reward: 30.84, Epsilon: 0.6626\n",
      "Episode 1300, Average reward: 36.6, Epsilon: 0.6519\n",
      "Episode 1350, Average reward: 33.5, Epsilon: 0.6413\n",
      "Episode 1400, Average reward: 28.22, Epsilon: 0.6308\n",
      "Episode 1450, Average reward: 32.6, Epsilon: 0.6206\n",
      "Episode 1500, Average reward: 40.02, Epsilon: 0.6105\n",
      "Episode 1550, Average reward: 31.74, Epsilon: 0.6005\n",
      "Episode 1600, Average reward: 33.7, Epsilon: 0.5908\n",
      "Episode 1650, Average reward: 31.86, Epsilon: 0.5812\n",
      "Episode 1700, Average reward: 32.14, Epsilon: 0.5717\n",
      "Episode 1750, Average reward: 37.44, Epsilon: 0.5625\n",
      "Episode 1800, Average reward: 41.54, Epsilon: 0.5533\n",
      "Episode 1850, Average reward: 36.56, Epsilon: 0.5443\n",
      "Episode 1900, Average reward: 36.7, Epsilon: 0.5355\n",
      "Episode 1950, Average reward: 41.42, Epsilon: 0.5268\n",
      "Episode 2000, Average reward: 43.84, Epsilon: 0.5183\n",
      "Episode 2050, Average reward: 35.6, Epsilon: 0.5099\n",
      "Episode 2100, Average reward: 40.74, Epsilon: 0.5016\n",
      "Episode 2150, Average reward: 53.76, Epsilon: 0.4935\n",
      "Episode 2200, Average reward: 51.38, Epsilon: 0.4855\n",
      "Episode 2250, Average reward: 33.7, Epsilon: 0.4776\n",
      "Episode 2300, Average reward: 34.84, Epsilon: 0.4699\n",
      "Episode 2350, Average reward: 44.56, Epsilon: 0.4623\n",
      "Episode 2400, Average reward: 33.2, Epsilon: 0.4548\n",
      "Episode 2450, Average reward: 38.94, Epsilon: 0.4475\n",
      "Episode 2500, Average reward: 42.92, Epsilon: 0.4403\n",
      "Episode 2550, Average reward: 47.9, Epsilon: 0.4331\n",
      "Episode 2600, Average reward: 48.88, Epsilon: 0.4261\n",
      "Episode 2650, Average reward: 54.1, Epsilon: 0.4193\n",
      "Episode 2700, Average reward: 58.76, Epsilon: 0.4125\n",
      "Episode 2750, Average reward: 52.4, Epsilon: 0.4059\n",
      "Episode 2800, Average reward: 52.56, Epsilon: 0.3993\n",
      "Episode 2850, Average reward: 41.86, Epsilon: 0.3929\n",
      "Episode 2900, Average reward: 46.12, Epsilon: 0.3865\n",
      "Episode 2950, Average reward: 42.78, Epsilon: 0.3803\n",
      "Episode 3000, Average reward: 47.86, Epsilon: 0.3742\n",
      "Episode 3050, Average reward: 51.42, Epsilon: 0.3682\n",
      "Episode 3100, Average reward: 49.14, Epsilon: 0.3623\n",
      "Episode 3150, Average reward: 50.88, Epsilon: 0.3564\n",
      "Episode 3200, Average reward: 43.88, Epsilon: 0.3507\n",
      "Episode 3250, Average reward: 44.54, Epsilon: 0.3451\n",
      "Episode 3300, Average reward: 42.26, Epsilon: 0.3395\n",
      "Episode 3350, Average reward: 48.9, Epsilon: 0.3341\n",
      "Episode 3400, Average reward: 56.1, Epsilon: 0.3287\n",
      "Episode 3450, Average reward: 47.0, Epsilon: 0.3235\n",
      "Episode 3500, Average reward: 45.26, Epsilon: 0.3183\n",
      "Episode 3550, Average reward: 39.62, Epsilon: 0.3132\n",
      "Episode 3600, Average reward: 40.6, Epsilon: 0.3082\n",
      "Episode 3650, Average reward: 34.92, Epsilon: 0.3033\n",
      "Episode 3700, Average reward: 37.3, Epsilon: 0.2984\n",
      "Episode 3750, Average reward: 45.96, Epsilon: 0.2936\n",
      "Episode 3800, Average reward: 41.6, Epsilon: 0.2890\n",
      "Episode 3850, Average reward: 45.94, Epsilon: 0.2843\n",
      "Episode 3900, Average reward: 48.26, Epsilon: 0.2798\n",
      "Episode 3950, Average reward: 47.46, Epsilon: 0.2753\n",
      "Episode 4000, Average reward: 49.8, Epsilon: 0.2710\n",
      "Episode 4050, Average reward: 54.5, Epsilon: 0.2666\n",
      "Episode 4100, Average reward: 54.54, Epsilon: 0.2624\n",
      "Episode 4150, Average reward: 56.4, Epsilon: 0.2582\n",
      "Episode 4200, Average reward: 47.56, Epsilon: 0.2541\n",
      "Episode 4250, Average reward: 52.74, Epsilon: 0.2501\n",
      "Episode 4300, Average reward: 52.14, Epsilon: 0.2461\n",
      "Episode 4350, Average reward: 53.66, Epsilon: 0.2422\n",
      "Episode 4400, Average reward: 52.14, Epsilon: 0.2384\n",
      "Episode 4450, Average reward: 78.64, Epsilon: 0.2346\n",
      "Episode 4500, Average reward: 59.04, Epsilon: 0.2309\n",
      "Episode 4550, Average reward: 48.52, Epsilon: 0.2272\n",
      "Episode 4600, Average reward: 55.26, Epsilon: 0.2237\n",
      "Episode 4650, Average reward: 57.26, Epsilon: 0.2201\n",
      "Episode 4700, Average reward: 68.22, Epsilon: 0.2167\n",
      "Episode 4750, Average reward: 67.12, Epsilon: 0.2132\n",
      "Episode 4800, Average reward: 65.74, Epsilon: 0.2099\n",
      "Episode 4850, Average reward: 63.04, Epsilon: 0.2066\n",
      "Episode 4900, Average reward: 68.94, Epsilon: 0.2033\n",
      "Episode 4950, Average reward: 41.2, Epsilon: 0.2001\n",
      "Episode 5000, Average reward: 66.3, Epsilon: 0.1970\n",
      "Episode 5050, Average reward: 58.48, Epsilon: 0.1939\n",
      "Episode 5100, Average reward: 72.44, Epsilon: 0.1909\n",
      "Episode 5150, Average reward: 67.36, Epsilon: 0.1879\n",
      "Episode 5200, Average reward: 60.16, Epsilon: 0.1849\n",
      "Episode 5250, Average reward: 117.86, Epsilon: 0.1820\n",
      "Episode 5300, Average reward: 76.9, Epsilon: 0.1792\n",
      "Episode 5350, Average reward: 68.08, Epsilon: 0.1764\n",
      "Episode 5400, Average reward: 75.66, Epsilon: 0.1736\n",
      "Episode 5450, Average reward: 75.14, Epsilon: 0.1709\n",
      "Episode 5500, Average reward: 80.52, Epsilon: 0.1683\n",
      "Episode 5550, Average reward: 88.82, Epsilon: 0.1657\n",
      "Episode 5600, Average reward: 88.66, Epsilon: 0.1631\n",
      "Episode 5650, Average reward: 87.86, Epsilon: 0.1606\n",
      "Episode 5700, Average reward: 106.36, Epsilon: 0.1581\n",
      "Episode 5750, Average reward: 81.44, Epsilon: 0.1556\n",
      "Episode 5800, Average reward: 79.82, Epsilon: 0.1532\n",
      "Episode 5850, Average reward: 92.22, Epsilon: 0.1509\n",
      "Episode 5900, Average reward: 84.28, Epsilon: 0.1485\n",
      "Episode 5950, Average reward: 46.14, Epsilon: 0.1462\n",
      "Episode 6000, Average reward: 64.8, Epsilon: 0.1440\n",
      "Episode 6050, Average reward: 92.14, Epsilon: 0.1418\n",
      "Episode 6100, Average reward: 80.42, Epsilon: 0.1396\n",
      "Episode 6150, Average reward: 93.66, Epsilon: 0.1374\n",
      "Episode 6200, Average reward: 83.86, Epsilon: 0.1353\n",
      "Episode 6250, Average reward: 95.4, Epsilon: 0.1333\n",
      "Episode 6300, Average reward: 71.04, Epsilon: 0.1312\n",
      "Episode 6350, Average reward: 88.14, Epsilon: 0.1292\n",
      "Episode 6400, Average reward: 68.06, Epsilon: 0.1273\n",
      "Episode 6450, Average reward: 52.56, Epsilon: 0.1253\n",
      "Episode 6500, Average reward: 44.38, Epsilon: 0.1234\n",
      "Episode 6550, Average reward: 55.54, Epsilon: 0.1215\n",
      "Episode 6600, Average reward: 49.1, Epsilon: 0.1197\n",
      "Episode 6650, Average reward: 62.6, Epsilon: 0.1179\n",
      "Episode 6700, Average reward: 63.62, Epsilon: 0.1161\n",
      "Episode 6750, Average reward: 69.12, Epsilon: 0.1143\n",
      "Episode 6800, Average reward: 38.2, Epsilon: 0.1126\n",
      "Episode 6850, Average reward: 30.18, Epsilon: 0.1109\n",
      "Episode 6900, Average reward: 51.7, Epsilon: 0.1093\n",
      "Episode 6950, Average reward: 52.7, Epsilon: 0.1076\n",
      "Episode 7000, Average reward: 43.74, Epsilon: 0.1060\n",
      "Episode 7050, Average reward: 47.6, Epsilon: 0.1044\n",
      "Episode 7100, Average reward: 57.76, Epsilon: 0.1029\n",
      "Episode 7150, Average reward: 49.38, Epsilon: 0.1013\n",
      "Episode 7200, Average reward: 46.08, Epsilon: 0.0998\n",
      "Episode 7250, Average reward: 57.86, Epsilon: 0.0983\n",
      "Episode 7300, Average reward: 51.7, Epsilon: 0.0969\n",
      "Episode 7350, Average reward: 49.7, Epsilon: 0.0954\n",
      "Episode 7400, Average reward: 43.38, Epsilon: 0.0940\n",
      "Episode 7450, Average reward: 46.86, Epsilon: 0.0926\n",
      "Episode 7500, Average reward: 53.2, Epsilon: 0.0913\n",
      "Episode 7550, Average reward: 52.18, Epsilon: 0.0899\n",
      "Episode 7600, Average reward: 46.86, Epsilon: 0.0886\n",
      "Episode 7650, Average reward: 44.52, Epsilon: 0.0873\n",
      "Episode 7700, Average reward: 52.14, Epsilon: 0.0860\n",
      "Episode 7750, Average reward: 41.04, Epsilon: 0.0848\n",
      "Episode 7800, Average reward: 43.08, Epsilon: 0.0835\n",
      "Episode 7850, Average reward: 44.86, Epsilon: 0.0823\n",
      "Episode 7900, Average reward: 40.96, Epsilon: 0.0811\n",
      "Episode 7950, Average reward: 54.68, Epsilon: 0.0799\n",
      "Episode 8000, Average reward: 46.1, Epsilon: 0.0788\n",
      "Episode 8050, Average reward: 46.22, Epsilon: 0.0777\n",
      "Episode 8100, Average reward: 49.58, Epsilon: 0.0765\n",
      "Episode 8150, Average reward: 50.02, Epsilon: 0.0754\n",
      "Episode 8200, Average reward: 48.98, Epsilon: 0.0744\n",
      "Episode 8250, Average reward: 49.24, Epsilon: 0.0733\n",
      "Episode 8300, Average reward: 58.18, Epsilon: 0.0722\n",
      "Episode 8350, Average reward: 43.98, Epsilon: 0.0712\n",
      "Episode 8400, Average reward: 48.9, Epsilon: 0.0702\n",
      "Episode 8450, Average reward: 54.2, Epsilon: 0.0692\n",
      "Episode 8500, Average reward: 42.2, Epsilon: 0.0682\n",
      "Episode 8550, Average reward: 62.06, Epsilon: 0.0673\n",
      "Episode 8600, Average reward: 41.12, Epsilon: 0.0663\n",
      "Episode 8650, Average reward: 55.94, Epsilon: 0.0654\n",
      "Episode 8700, Average reward: 53.78, Epsilon: 0.0645\n",
      "Episode 8750, Average reward: 59.46, Epsilon: 0.0636\n",
      "Episode 8800, Average reward: 52.0, Epsilon: 0.0627\n",
      "Episode 8850, Average reward: 41.26, Epsilon: 0.0618\n",
      "Episode 8900, Average reward: 49.36, Epsilon: 0.0610\n",
      "Episode 8950, Average reward: 52.06, Epsilon: 0.0601\n",
      "Episode 9000, Average reward: 40.9, Epsilon: 0.0593\n",
      "Episode 9050, Average reward: 49.58, Epsilon: 0.0585\n",
      "Episode 9100, Average reward: 45.9, Epsilon: 0.0577\n",
      "Episode 9150, Average reward: 51.96, Epsilon: 0.0569\n",
      "Episode 9200, Average reward: 40.18, Epsilon: 0.0561\n",
      "Episode 9250, Average reward: 51.24, Epsilon: 0.0553\n",
      "Episode 9300, Average reward: 49.5, Epsilon: 0.0546\n",
      "Episode 9350, Average reward: 41.76, Epsilon: 0.0539\n",
      "Episode 9400, Average reward: 52.98, Epsilon: 0.0531\n",
      "Episode 9450, Average reward: 44.22, Epsilon: 0.0524\n",
      "Episode 9500, Average reward: 52.02, Epsilon: 0.0517\n",
      "Episode 9550, Average reward: 47.18, Epsilon: 0.0510\n",
      "Episode 9600, Average reward: 50.6, Epsilon: 0.0504\n",
      "Episode 9650, Average reward: 48.56, Epsilon: 0.0497\n",
      "Episode 9700, Average reward: 50.68, Epsilon: 0.0490\n",
      "Episode 9750, Average reward: 49.6, Epsilon: 0.0484\n",
      "Episode 9800, Average reward: 35.78, Epsilon: 0.0478\n",
      "Episode 9850, Average reward: 44.72, Epsilon: 0.0471\n",
      "Episode 9900, Average reward: 53.18, Epsilon: 0.0465\n",
      "Episode 9950, Average reward: 38.86, Epsilon: 0.0459\n"
     ]
    }
   ],
   "source": [
    "q_table, total_episode_states = train_agent(\n",
    "    n_episodes=10000, \n",
    "    alpha=0.05, \n",
    "    gamma=0.99, \n",
    "    epsilon_start=1.0, \n",
    "    epsilon_end=0.01, \n",
    "    epsilon_decay=3000\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "b9588982",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Episode 1/10, Length: 65\n",
      "Test Episode 2/10, Length: 58\n",
      "Test Episode 3/10, Length: 15\n",
      "Test Episode 4/10, Length: 63\n",
      "Test Episode 5/10, Length: 111\n",
      "Test Episode 6/10, Length: 22\n",
      "Test Episode 7/10, Length: 17\n",
      "Test Episode 8/10, Length: 22\n",
      "Test Episode 9/10, Length: 11\n",
      "Test Episode 10/10, Length: 22\n",
      "Average episode length: 40.60\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "40.6"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_agent(q_table)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "789a8405",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "rlagent",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
