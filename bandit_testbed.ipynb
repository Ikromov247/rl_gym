{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d73ccb88",
   "metadata": {},
   "source": [
    "Exercise 2.5 (programming) Design and conduct an experiment to demonstrate the </br>\n",
    "difficulties that sample-average methods have for nonstationary problems. Use a modified </br>\n",
    "version of the 10-armed testbed in which all the q⇤(a) start out equal and then take</br>\n",
    "independent random walks (say by adding a normally distributed increment with mean </br>\n",
    "zero and standard deviation 0.01 to all the q⇤(a) on each step). Prepare plots like</br>\n",
    "Figure 2.2 for an action-value method using sample averages, incrementally computed,</br>\n",
    "and another action-value method using a constant step-size parameter, a = 0.1. Use </br>\n",
    "eps = 0.1 and longer runs, say of 10,000 steps."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cada08e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8322dfaf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialize rewards\n",
    "def assign_mean_rewards(n_actions, mean, variance):\n",
    "    return np.random.normal(mean, variance, size=n_actions)\n",
    "\n",
    "def assign_equal_rewards(n_actions):\n",
    "    return np.zeros(n_actions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32e08751",
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculate reward\n",
    "def get_reward(true_rewards, action, variance):\n",
    "    reward_mean = true_rewards[action]\n",
    "    return np.random.normal(reward_mean, variance)\n",
    "\n",
    "def get_drifting_reward(true_rewards, action):\n",
    "    \"\"\"Drift all the rewards\"\"\"\n",
    "    n = len(true_rewards)\n",
    "    drifted_rewards = true_rewards + np.random.normal(0, 0.01, size=n)\n",
    "    return drifted_rewards, true_rewards[action]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53295a2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# policies\n",
    "def greedy(qtable):\n",
    "    return np.argmax(qtable)\n",
    "\n",
    "def epsilon_greedy(qtable, n_actions, epsilon):\n",
    "    if np.random.random() < epsilon:\n",
    "        return np.random.randint(0, n_actions)\n",
    "    else:\n",
    "        return np.argmax(qtable)\n",
    "\n",
    "def ucb(qtable:np.ndarray, counts:np.ndarray, timestamp:int, exploration_rate:float):\n",
    "    # argmax(qtable + c * ((ln(timestamp) / n)**0.5) )\n",
    "    # first try all actions once\n",
    "    zero_mask = (counts == 0)\n",
    "    if zero_mask.any():\n",
    "        # pick the first untried action\n",
    "        a = int(np.flatnonzero(zero_mask)[0])\n",
    "        counts[a] += 1\n",
    "        return a, counts\n",
    "    \n",
    "    uncertainty = (np.log(timestamp+1) / counts)**0.5\n",
    "    chosen_action  = int(\n",
    "        np.argmax(qtable + exploration_rate * uncertainty)\n",
    "    )\n",
    "    counts[chosen_action] += 1\n",
    "\n",
    "    return chosen_action, counts\n",
    "\n",
    "def gradient_bandit(preferences:np.ndarray):\n",
    "    \"\"\"Calculates probabilities of all actions based on their `preference` value\"\"\"\n",
    "    probabilities = []\n",
    "    for action in (range(len(preferences))):\n",
    "        prob = np.exp(preferences[action]) / np.sum(np.exp(preferences))\n",
    "        probabilities.append(prob)\n",
    "    return probabilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a08a0cc4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# qtable updates\n",
    "def update_estimate_average(qtable, true_reward, action, timestamp):\n",
    "    current_estimate = qtable[action]\n",
    "    # update our reward estimate by sample averaging\n",
    "    updated_estimate = current_estimate + (true_reward - current_estimate) / (timestamp + 1)\n",
    "    qtable[action] = updated_estimate\n",
    "    return None\n",
    "\n",
    "def update_estimate_alpha(qtable, true_reward, action, alpha):\n",
    "    current_estimate = qtable[action]\n",
    "    # update our reward estimate by alpha\n",
    "    updated_estimate = current_estimate + alpha * (true_reward - current_estimate)\n",
    "    qtable[action] = updated_estimate\n",
    "    return None\n",
    "\n",
    "def update_unbiased_constant(qtable, true_reward, action, timestamp, alpha):\n",
    "    # formula to calculate trace at timestamp t: `1 + (1-alpha)^t  * (trace_0 - 1)`\n",
    "    # where trace_0 is an initial constant trace value. In the textbook, it is set as 0 \n",
    "    # which leaves us with the below formula:\n",
    "    # 1 + (1-alpha) * (0-1) -> 1 + (1-alpha)^t * (-1) -> 1 - (1-alpha)^t\n",
    "    trace = 1 - (1-alpha)**timestamp\n",
    "    step_size = alpha / trace\n",
    "\n",
    "    current_estimate = qtable[action]\n",
    "    updated_estimate = current_estimate + step_size * (true_reward - current_estimate)\n",
    "    qtable[action] = updated_estimate\n",
    "    return None\n",
    "\n",
    "def update_gradient(preferences, probabilities, true_reward, action, alpha, reward_baseline):\n",
    "    # update preference for chosen action\n",
    "    for a in range(len(preferences)):\n",
    "        current_preference = preferences[a]\n",
    "        if a == action:\n",
    "            preferences[action] = current_preference + alpha * (true_reward - reward_baseline) * (1 - probabilities[a])\n",
    "        else:\n",
    "            preferences[action] = current_preference - alpha * (true_reward - reward_baseline) * probabilities[a]\n",
    "\n",
    "def update_reward_baseline(baseline, new_reward, drifting, step_size):\n",
    "    if drifting:\n",
    "        return baseline + step_size * (new_reward - baseline)\n",
    "    else:\n",
    "        return baseline + (new_reward - baseline) / (step_size + 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e38c2300",
   "metadata": {},
   "outputs": [],
   "source": [
    "# plotting\n",
    "def plot_run_results(rewards):\n",
    "    plt.figure(figsize=(12, 7))\n",
    "    plt.plot(\n",
    "        np.arange(len(rewards)), rewards\n",
    "    )\n",
    "    plt.title(\"Rewards over time\")\n",
    "    plt.show()\n",
    "\n",
    "def plot_many_run_results(title:str, **kwargs):\n",
    "    plt.figure(figsize=(12, 7))\n",
    "    \n",
    "    first_col = list(kwargs.keys())[0]\n",
    "    x_axis = np.arange(len(kwargs[first_col])) \n",
    "    \n",
    "    for name, values in kwargs.items():\n",
    "        plt.plot(\n",
    "            x_axis, values, label=name\n",
    "        )\n",
    "    \n",
    "    plt.title(title)\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "\n",
    "def plot_optimization_results(title:str, **kwargs):\n",
    "    plt.figure(figsize=(12, 7))\n",
    "    \n",
    "    policies = list(kwargs.keys())\n",
    "    \n",
    "    for policy in policies:\n",
    "        x_axis = list(kwargs[policy].keys())\n",
    "        values = list(kwargs[policy].values())\n",
    "        plt.plot(\n",
    "            x_axis, values, label=policy\n",
    "        )\n",
    "    \n",
    "    plt.title(title)\n",
    "    plt.legend()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9b2a811",
   "metadata": {},
   "outputs": [],
   "source": [
    "def experiment(\n",
    "        narms = 10,\n",
    "        nruns = 2_000,\n",
    "        eps = 0.01,\n",
    "        alpha = 0.1,\n",
    "        timestamps = 10_000,\n",
    "        initial_reward = 0,\n",
    "        policy='epsilon', # ucb, epsilon, greedy\n",
    "        exploration_rate:int=2,\n",
    "        drifting=True,\n",
    "        variance=1,\n",
    "\n",
    "        ):\n",
    "    print(\n",
    "        f\"Running optimization for policy {policy} with parameters: {locals()}\"\n",
    "    )\n",
    "    total_rewards = []\n",
    "    total_choices = []\n",
    "\n",
    "    if policy == 'ucb':\n",
    "        counts = np.zeros(narms)\n",
    "    elif policy=='gradient':\n",
    "        reward_baseline = 0\n",
    "\n",
    "    for n in range(nruns):\n",
    "        # actual mean rewards for each action\n",
    "        rewards_table = assign_equal_rewards(narms)\n",
    "        \n",
    "        # agent's estimate\n",
    "        qtable = np.zeros(narms) + initial_reward # optimistic initialization\n",
    "        bandit_rewards = []\n",
    "        optimal_action = []\n",
    "\n",
    "        for i in range(timestamps):\n",
    "            # policies here\n",
    "            if policy == 'epsilon':\n",
    "                chosen_action = epsilon_greedy(qtable, narms, eps)\n",
    "            elif policy == 'ucb':\n",
    "                chosen_action, counts = ucb(qtable, counts, i, exploration_rate)\n",
    "            elif policy == 'gradient':\n",
    "                probabilities = gradient_bandit(qtable)\n",
    "                chosen_action = np.argmax(probabilities)\n",
    "            elif policy == 'greedy':\n",
    "                chosen_action = greedy(qtable)\n",
    "            \n",
    "            if drifting:\n",
    "                rewards_table, reward = get_drifting_reward(rewards_table, chosen_action)\n",
    "            else:\n",
    "                rewards_table, reward = get_reward(rewards_table, chosen_action, variance)\n",
    "\n",
    "            if policy == 'gradient':\n",
    "                update_gradient(qtable, probabilities, reward, chosen_action, alpha, reward_baseline)\n",
    "                # update reward baseline for non-stationary problem\n",
    "                step_size = alpha if drifting else i\n",
    "                reward_baseline = update_reward_baseline(reward_baseline, reward, drifting, step_size)\n",
    "            else:\n",
    "                update_estimate_alpha(qtable, reward, chosen_action, alpha)\n",
    "\n",
    "            optimal_action.append(\n",
    "                int(chosen_action == np.argmax(rewards_table))\n",
    "            )\n",
    "            bandit_rewards.append(reward)\n",
    "        \n",
    "        total_rewards.append(bandit_rewards)\n",
    "        total_choices.append(optimal_action)\n",
    "        if n % (nruns // 10)==0 and n > 0:\n",
    "            mean_reward = np.mean(total_rewards)\n",
    "            print(\n",
    "                f\"Mean reward: {mean_reward} \"\n",
    "            ) \n",
    "    return total_rewards, total_choices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78009306",
   "metadata": {},
   "outputs": [],
   "source": [
    "narms = 10\n",
    "nruns = 2_000\n",
    "alpha = 0.1 # only changes for gradient bandit \n",
    "timestamps = 10_000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03940624",
   "metadata": {},
   "outputs": [],
   "source": [
    "hyper_param_list = [2 ** x for x in range(-7, 3)]\n",
    "\n",
    "mean_rewards = {}\n",
    "mean_optimal_choices = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be2a4a1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "policy = 'epsilon'\n",
    "mean_rewards[policy] = {}\n",
    "mean_optimal_choices[policy] = {}\n",
    "\n",
    "for eps in hyper_param_list:\n",
    "    if eps >= 1:\n",
    "        break\n",
    "    total_rewards, total_choices = experiment(\n",
    "        narms=narms,\n",
    "        nruns=nruns,\n",
    "        eps=eps, # <-- hyper-parameter\n",
    "        alpha=alpha,\n",
    "        timestamps=timestamps,\n",
    "        policy=policy,\n",
    "\n",
    "    )\n",
    "    # only consider the second half of timestamps for reward calculation\n",
    "    mean_reward = np.mean(total_rewards[timestamps//2:]) \n",
    "    optimal_choice = np.mean(total_choices[timestamps//2:], axis=0) * 100.0\n",
    "\n",
    "    mean_rewards[policy][eps] = mean_reward\n",
    "    mean_optimal_choices[policy][eps] = mean_reward\n",
    "\n",
    "# greedy\n",
    "policy = 'greedy'\n",
    "mean_rewards[policy] = {}\n",
    "mean_optimal_choices[policy] = {}\n",
    "\n",
    "for initial_reward in hyper_param_list:\n",
    "    if initial_reward < 0.125:\n",
    "        continue\n",
    "    total_rewards, total_choices = experiment(\n",
    "        narms=narms,\n",
    "        nruns=nruns,\n",
    "        eps=None,\n",
    "        alpha=alpha,\n",
    "        timestamps=timestamps,\n",
    "        policy=policy,\n",
    "        initial_reward=initial_reward,\n",
    "        \n",
    "    )\n",
    "\n",
    "    # only consider the second half of timestamps for reward calculation\n",
    "    mean_reward = np.mean(total_rewards[timestamps//2:]) \n",
    "    optimal_choice = np.mean(total_choices[timestamps//2:], axis=0) * 100.0\n",
    "\n",
    "    mean_rewards[policy][initial_reward] = mean_reward\n",
    "    mean_optimal_choices[policy][initial_reward] = mean_reward\n",
    "\n",
    "\n",
    "# ucb\n",
    "policy = 'ucb'\n",
    "mean_rewards[policy] = {}\n",
    "mean_optimal_choices[policy] = {}\n",
    "for exploration_rate in hyper_param_list:\n",
    "    if exploration_rate < 0.25:\n",
    "        continue\n",
    "    total_rewards, total_choices = experiment(\n",
    "        narms=narms,\n",
    "        nruns=nruns,\n",
    "        eps=None,\n",
    "        alpha=alpha,\n",
    "        timestamps=timestamps,\n",
    "        policy=policy,\n",
    "        exploration_rate=exploration_rate,\n",
    "        \n",
    "    )\n",
    "\n",
    "    # only consider the second half of timestamps for reward calculation\n",
    "    mean_reward = np.mean(total_rewards[timestamps//2:]) \n",
    "    optimal_choice = np.mean(total_choices[timestamps//2:], axis=0) * 100.0\n",
    "\n",
    "    mean_rewards[policy][exploration_rate] = mean_reward\n",
    "    mean_optimal_choices[policy][exploration_rate] = mean_reward\n",
    "\n",
    "\n",
    "# gradient\n",
    "policy = 'gradient'\n",
    "mean_rewards[policy] = {}\n",
    "mean_optimal_choices[policy] = {}\n",
    "for alpha in hyper_param_list:\n",
    "\n",
    "    total_rewards, total_choices = experiment(\n",
    "        narms=narms,\n",
    "        nruns=nruns,\n",
    "        eps=None,\n",
    "        alpha=alpha,\n",
    "        timestamps=timestamps,\n",
    "        policy=policy,\n",
    "        initial_reward=1, # just to avoid a 0 \n",
    "\n",
    "    )\n",
    "\n",
    "    # only consider the second half of timestamps for reward calculation\n",
    "    mean_reward = np.mean(total_rewards[timestamps//2:]) \n",
    "    optimal_choice = np.mean(total_choices[timestamps//2:], axis=0) * 100.0\n",
    "\n",
    "    mean_rewards[policy][alpha] = mean_reward\n",
    "    mean_optimal_choices[policy][alpha] = mean_reward\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6fdfcbd2",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_optimization_results(\n",
    "    title='optimization comparison',\n",
    "    **mean_rewards\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d05b701b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0d3d952",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "rlagent",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
